https://www.reddit.com/r/ollama/comments/1ibhxvm/guide_to_installing_and_locally_running_ollama/?tl=it

https://www.reddit.com/r/ollama/search/?q=Ollama+UIs&cId=db9cec72-08df-4653-9f90-75fef40719ad&iId=7c1b71c8-8ec9-4a48-a355-360ba7efe6cf

https://www.reddit.com/r/ollama/comments/1ki7x1s/new_very_simple_ui_for_ollama/



chrome extensio per ollama ui
https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo
repo:
https://github.com/n4ze3m/page-assist

LISTA DI TUTORIAL
https://www.reddit.com/r/ollama/search/?q=tutorial&cId=36c190ee-041c-4f59-a4ad-cac57aea6bb9&iId=fb0a2b43-3aa2-4e7a-80ab-575906ee5456


Ecco una lista completa con le descrizioni di Ollama dedicate agli sviluppatori software, affiancate dai link diretti alle risorse più utili per approfondire e iniziare a usare la piattaforma:

- **Ollama**: framework leggero per eseguire modelli di linguaggio locali di grandi dimensioni (LLM) come Llama 2, Code Llama, Mistral e altri.
    
    - Documentazione ufficiale principale: [https://ollama.com](https://ollama.com/)[](https://ollama.com/)
    - Docker https://hub.docker.com/r/ollama/ollama
        
- **Documentazione tecnica e guida rapida (Quickstart)**: istruzioni base su come scaricare, eseguire e personalizzare modelli con Ollama.
    
    - Quickstart ufficiale: [https://ollama.readthedocs.io/en/quickstart/](https://ollama.readthedocs.io/en/quickstart/)[](https://ollama.readthedocs.io/en/quickstart/)
        
- **API REST di Ollama**: endpoint per interagire programmaticamente con i modelli, tra cui generazione di completamenti, chat, gestione modelli e streaming delle risposte.
    
    - Documentazione API REST dettagliata: [https://ollama.readthedocs.io/en/api/](https://ollama.readthedocs.io/en/api/)[](https://ollama.readthedocs.io/en/api/)
        
    - API Postman: [https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api](https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api)[](https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api)
        
- **SDK Ollama per Python**: libreria Python per integrare facilmente Ollama con pochi comandi, supporta esecuzione di modelli, definizione di funzioni di supporto e altro.
    
    - Guida passo passo Python: [https://www.cohorte.co/blog/using-ollama-with-python-step-by-step-guide](https://www.cohorte.co/blog/using-ollama-with-python-step-by-step-guide)[](https://www.cohorte.co/blog/using-ollama-with-python-step-by-step-guide)
        
    - Repositry ufficiale e libreria Python: [https://github.com/ollama/ollama-python](https://github.com/ollama/ollama-python)[](https://github.com/ollama/ollama-python)
        
    - PyPI ollama-python package: [https://pypi.org/project/ollama-python/](https://pypi.org/project/ollama-python/)[](https://pypi.org/project/ollama-python/)
        
- **SDK Ollama per JavaScript**: libreria JS per accedere alle API Ollama da progetti JavaScript o Node.js, con esempi di uso e supporto per streaming delle risposte.
    
    - Repository ufficiale JS: [https://github.com/ollama/ollama-js](https://github.com/ollama/ollama-js)[](https://github.com/ollama/ollama-js)
        
    - Wrapper JS di terze parti: [https://github.com/dditlev/ollama-js-client](https://github.com/dditlev/ollama-js-client)[](https://github.com/dditlev/ollama-js-client)
        
- **CLI Ollama**: terminale e comando per gestire modelli, eseguire modelli, creare modelli personalizzati, scaricare modelli, avviare sessioni interattive.
    
    - Guida tutorial CLI: [https://www.hostinger.com/tutorials/ollama-cli-tutorial](https://www.hostinger.com/tutorials/ollama-cli-tutorial)[](https://www.hostinger.com/tutorials/ollama-cli-tutorial)
        
- **Supporto GPU e ottimizzazione**: guida e consigli su come usare GPU NVIDIA e AMD per migliorare prestazioni di Ollama in locale.
    
    - Guida supporto AMD GPU: [https://www.amd.com/en/developer/resources/technical-articles/running-llms-locally-on-amd-gpus-with-ollama.html](https://www.amd.com/en/developer/resources/technical-articles/running-llms-locally-on-amd-gpus-with-ollama.html)[](https://www.amd.com/en/developer/resources/technical-articles/running-llms-locally-on-amd-gpus-with-ollama.html)
        
    - Ottimizzazione Ubuntu per GPU non ufficialmente supportate: [https://www.conroyp.com/articles/running-ollama-ubuntu-unsupported-amd-gpu-performance-guide](https://www.conroyp.com/articles/running-ollama-ubuntu-unsupported-amd-gpu-performance-guide)[](https://www.conroyp.com/articles/running-ollama-ubuntu-unsupported-amd-gpu-performance-guide)
        
- **Output strutturati e supporto tool avanzati**: possibilità di definire output in schemi JSON e usare chiamate a funzioni esterne per potenziare i modelli.
    
    - Blog su output strutturati: [https://ollama.com/blog/structured-outputs](https://ollama.com/blog/structured-outputs)[](https://ollama.com/blog/structured-outputs)
        
    - Blog tool support: [https://ollama.com/blog/tool-support](https://ollama.com/blog/tool-support)[](https://ollama.com/blog/tool-support)
        
- **Documentazione generale dettagliata**: raccolta completa di guide, riferimento API, esempi, installazione e uso su diversi sistemi operativi.
    
    - Pagina docs principale: [https://ollama.readthedocs.io/en/](https://ollama.readthedocs.io/en/)[](https://ollama.readthedocs.io/en/)